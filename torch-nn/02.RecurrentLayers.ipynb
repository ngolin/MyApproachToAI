{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7a7b1f",
   "metadata": {},
   "source": [
    "## Recurrent Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdceaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20945bd4",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d388f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.RNN(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "    num_layers=C=1,\n",
    "    batch_first={(L, N?): False, (N?, L): True}[B],\n",
    "    bidirectional={1: False, 2: True}[D]\n",
    "):  (*B, Hin),    (D*C, N?, Hout)\n",
    "->  (*B, D*Hout), (D*C, N?, Hout)\n",
    "\"\"\"\n",
    "\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "# https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, num_layers, nonlinearity, bias = 2, 3, 4, \"tanh\", True\n",
    "\n",
    "\"\"\"2. Keyword Arguments\"\"\"\n",
    "batch_first, bidirectional = False, False\n",
    "\n",
    "rnn = nn.RNN(\n",
    "    input_size,  # Hin, Required\n",
    "    hidden_size,  # Hout, Required\n",
    "    num_layers,  # C, default=1\n",
    "    nonlinearity,  # \"tanh\" or \"relu\", default=\"tanh\"\n",
    "    bias,  # default=True\n",
    "    batch_first=batch_first,  # default=False\n",
    "    dropout=0.0,\n",
    "    bidirectional=bidirectional,  # default=False\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size, seq_len = 5, 6\n",
    "\n",
    "if batch_first:\n",
    "    x = torch.randn(batch_size, seq_len, input_size)  # (N, L, Hin)\n",
    "else:\n",
    "    x = torch.randn(seq_len, batch_size, input_size)  # (L, N, Hin)\n",
    "\n",
    "h0 = torch.randn(D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "y, h = rnn(x, None if h0 is None else h0)\n",
    "\n",
    "if batch_first:\n",
    "    assert y.shape == (batch_size, seq_len, D * hidden_size)  # (N, L, D * Hout)\n",
    "else:\n",
    "    assert y.shape == (seq_len, batch_size, D * hidden_size)  # (L, N, D * Hout)\n",
    "\n",
    "assert h.shape == (D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbaa05",
   "metadata": {},
   "source": [
    "### RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1c7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.RNNCell(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "):  (N?, Hin),  (N?, Hout)\n",
    "->  (N?, Hout), (N?, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, bias, nonlinearity = 2, 3, True, \"tanh\"\n",
    "\n",
    "\n",
    "rnn_cell = nn.RNNCell(\n",
    "    input_size,  # Hin, Required\n",
    "    hidden_size,  # Hout, Required\n",
    "    bias,  # default=True\n",
    "    nonlinearity,  # \"tanh\" or \"relu\", default=\"tanh\"\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "f = {\"tanh\": F.tanh, \"relu\": F.relu}[nonlinearity]\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size = 4\n",
    "\n",
    "x = torch.randn(batch_size, input_size)  # (N, Hin)\n",
    "\n",
    "h0 = torch.randn(batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "h = rnn_cell(x, None if h0 is None else h0)\n",
    "\n",
    "assert h.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "assert torch.allclose(\n",
    "    h,\n",
    "    f(\n",
    "        F.linear(x, rnn_cell.weight_ih, rnn_cell.bias_ih)\n",
    "        + F.linear(h0, rnn_cell.weight_hh, rnn_cell.bias_hh)\n",
    "    ),\n",
    "    atol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4815dce",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f21806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.LSTM(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "    num_layers=C=1,\n",
    "    batch_first={(L, N?): False, (N?, L): True}[B],\n",
    "    bidirectional={1: False, 2: True}[D],\n",
    "    proj_size=P=P if P > 0 else Hout\n",
    "):  (*B, Hin), ((D*C, N?, P), (D*C, N?, Hout))\n",
    "->  (*B, D*P), ((D*C, N?, P), (D*C, N?, Hout))\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "# https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, num_layers, bias = 2, 3, 1, True\n",
    "\n",
    "\"\"\"2. Keyword Arguments\"\"\"\n",
    "batch_first, bidirectional, proj_size = False, True, 1\n",
    "\n",
    "assert proj_size < hidden_size, \"proj_size has to be smaller than hidden_size\"\n",
    "\n",
    "lstm = nn.LSTM(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    num_layers,  # C\n",
    "    bias=bias,\n",
    "    batch_first=batch_first,\n",
    "    dropout=0.0,\n",
    "    bidirectional=bidirectional,\n",
    "    proj_size=proj_size,  # default=0\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "D = 2 if bidirectional else 1\n",
    "P = proj_size if proj_size > 0 else hidden_size\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size, seq_len = 4, 5\n",
    "\n",
    "if batch_first:\n",
    "    x = torch.randn(batch_size, seq_len, input_size)  # (N, L, Hin)\n",
    "else:\n",
    "    x = torch.randn(seq_len, batch_size, input_size)  # (L, N, Hin)\n",
    "\n",
    "h = torch.randn(D * num_layers, batch_size, P)  # (D * C, N, P)\n",
    "c = torch.randn(D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "y, (h, c) = lstm(x, None if h is None or c is None else (h, c))\n",
    "\n",
    "if batch_first:\n",
    "    assert y.shape == (batch_size, seq_len, D * P)  # (N, L, D * P)\n",
    "else:\n",
    "    assert y.shape == (seq_len, batch_size, D * P)  # (L, N, D * P)\n",
    "\n",
    "assert h.shape == (D * num_layers, batch_size, P)  # (D * C, N, P)\n",
    "assert c.shape == (D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b42c94",
   "metadata": {},
   "source": [
    "### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8973ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.LSTMCell(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "):  (N?, Hin), ((N?, P), (N?, Hout))\n",
    "->  (N?, P),   ((N?, P), (N?, Hout))\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "# https://yb.tencent.com/s/4OGnkvsVzqDH\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, bias = 2, 3, True\n",
    "\n",
    "lstm_cell = nn.LSTMCell(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    bias,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size = 4\n",
    "\n",
    "x = torch.randn(batch_size, input_size)  # (N, Hin)\n",
    "\n",
    "h0 = torch.randn(batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "c0 = torch.randn(batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "h, c = lstm_cell(x, None if h0 is None or c0 is None else (h0, c0))\n",
    "\n",
    "assert h.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "assert c.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "gates = F.linear(x, lstm_cell.weight_ih, lstm_cell.bias_ih) + F.linear(\n",
    "    h0, lstm_cell.weight_hh, lstm_cell.bias_hh\n",
    ")\n",
    "i, f, g, o = gates.chunk(4, dim=1)  # Split into input, forget, gate and output\n",
    "i = F.sigmoid(i)\n",
    "f = F.sigmoid(f)\n",
    "g = F.tanh(g)\n",
    "o = F.sigmoid(o)\n",
    "\n",
    "c1 = f * c0 + i * g  # Update cell state\n",
    "h1 = o * F.tanh(c)  # Update hidden state\n",
    "\n",
    "assert torch.allclose(h, h1, atol=1e-6)\n",
    "assert torch.allclose(c, c1, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d330e1d",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb75245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.GRU(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "    num_layers=C=1,\n",
    "    batch_first={(L, N?): False, (N?, L): True}[B],\n",
    "    bidirectional={1: False, 2: True}[D]\n",
    "):  (*B, Hin),    (D*C, N?, Hout)\n",
    "->  (*B, D*Hout), (D*C, N?, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, num_layers, bias = 2, 3, 1, True\n",
    "\n",
    "\"\"\"2. Keyword Arguments\"\"\"\n",
    "batch_first, bidirectional = False, True\n",
    "\n",
    "gru = nn.GRU(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    num_layers,  # C\n",
    "    bias,\n",
    "    batch_first=batch_first,\n",
    "    dropout=0.0,\n",
    "    bidirectional=bidirectional,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size, seq_len = 4, 5\n",
    "\n",
    "if batch_first:\n",
    "    x = torch.randn(batch_size, seq_len, input_size)  # (N, L, Hin)\n",
    "else:\n",
    "    x = torch.randn(seq_len, batch_size, input_size)  # (L, N, Hin)\n",
    "\n",
    "h = torch.randn(D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "y, h = gru(x, None if h is None else h)\n",
    "\n",
    "if batch_first:\n",
    "    assert y.shape == (batch_size, seq_len, D * hidden_size)  # (N, L, D * Hout)\n",
    "else:\n",
    "    assert y.shape == (seq_len, batch_size, D * hidden_size)  # (L, N, D * Hout)\n",
    "\n",
    "assert h.shape == (D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420f3dd",
   "metadata": {},
   "source": [
    "### GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96cfdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.GRUCell(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "):  (N?, Hin),  (N?, Hout)\n",
    "->  (N?, Hout), (N?, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.GRUCell.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, bias = 2, 3, True\n",
    "\n",
    "gru_cell = nn.GRUCell(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    bias=bias,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size = 4\n",
    "\n",
    "x0 = torch.randn(batch_size, input_size)  # (N, Hin)\n",
    "\n",
    "h0 = torch.randn(batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "h = gru_cell(x0, None if h0 is None else h0)\n",
    "\n",
    "assert h.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "x_gates = F.linear(x0, gru_cell.weight_ih, gru_cell.bias_ih).chunk(3, dim=1)\n",
    "h_gates = F.linear(h0, gru_cell.weight_hh, gru_cell.bias_hh).chunk(3, dim=1)\n",
    "r = F.sigmoid(x_gates[0] + h_gates[0])  # Reset gate\n",
    "z = F.sigmoid(x_gates[1] + h_gates[1])  # Update gate\n",
    "n = F.tanh(x_gates[2] + r * h_gates[2])  # New gate\n",
    "h1 = (torch.ones_like(z) - z) * n + z * h0  # Update hidden state\n",
    "\n",
    "assert torch.allclose(h, h1, atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
